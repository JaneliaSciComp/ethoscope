#@include 
NULL
#' Read data from a result file.
#' 
#' This function is used to convert all the information
#' contained in a result file generated by PSV into an R data.table.
#'
#' @param FILE the name of the input file.
#' @param rois a vector of regions of interest (ROI) indices to read. NULL means all ROIs
#' @param min_time exclude data before min_time (in seconds). This time is relative to the start of the experiement.
#' @param max_time exclude data after max_time (in seconds). This time is relative to the start of the experiement.
#' @param condition_df an optionnal dataframe to provide experiemental conditions for each ROI. 
#' This dataframe should have one row per ROI and, at least, a \code{roi_id} column.
#' @param relative_distance whether distance is converted between 0 and 1, relative to the width of the ROI and where 0.0 is the top-left corner of the ROI. 
#' If FALSE, varibales such as x,y, w and h are returned is returned in absolute number of pixels.
#' @param time_in_seconds whether time is expressed in seconds (TRUE) or milliseconds (FALSE).
#' @param reference_hour the hour, in the day, to use as t0 reference.
#' @param add_file_name whether to add the name of the input file as an extra column.
#' @param FUN an optionnal function to be applied to the resulting dataframe. It can be used, for instance, to compute summary statistics, or to transform the data.
#' @param ... extra arguments to be passed to \code{FUN}
#' @return If \code{rois} has only one element, a dataframe. Otherwise, a list of dataframes (one per ROI)
#' @note Analysis of many long (sevaral days) recording can use a large amount of RAM. 
#' Therefore, it can sometimes be advantageaous to load an process ROIs one by one.
#' @examples
#' \dontrun{
#' FILE <- "result.db"
#' out <- loadROIsFromFile(FILE,c(1,3,55))
#' #histogram of x marginal distribution
#' hist(out[roi_id == 1, x], nclass=100)
#' }
#' \dontrun{
#' # More realistec example where we have experiemental conditions, and 
#' we want to resample data at 1.0Hz.
#' # First, the conditions:
#' conditions <- cbind(roi_id=1:32, expand.grid(treatment=c(T,F), genotype=LETTERS[1:4]))
#' print(conditions)
#'
#' # Then we apply the interpolateROIData to all ROIs:
#'  out <- loadROIsFromFile(FILE, 
#' 	FUN=interpolateROIData, start=0,
#'  conditions=conditions, fs=1)
#' 	}
#' @seealso \code{\link{loadMetaData}} To display global informations about the experiment. 
#' @export
loadROIsFromFile <- function(FILE, rois = NULL, min_time = 0,
				max_time = Inf, relative_distances = TRUE,
				condition_df = NULL, time_in_seconds=TRUE,
				reference_hour=NULL,
				add_file_name=FALSE,
				FUN=NULL,
				...){
					
	metadata <- loadMetaData(FILE)
	
	con <- dbConnect(SQLite(), FILE)
	roi_map <- as.data.table(dbGetQuery(con, "SELECT * FROM ROI_MAP"))
	var_map <- as.data.table(dbGetQuery(con, "SELECT * FROM VAR_MAP"))
	
	setkey(roi_map, roi_idx)
	setkey(var_map, var_name)
	
	available_rois  <- roi_map[ ,roi_idx]
	
	if(is.null(rois))
		rois <- available_rois
	
	matched <- rois %in% available_rois
	unmatched_idx <-  which(!matched)
	
	for(i in rois[unmatched_idx])
		warning(sprintf("Roi %i is not in the table", i))
	
	rois <- rois[matched]
	
	if (length(rois) == 0)
		stop(sprintf("No ROI to be read. available ROIs are: %s", str(sort(available_rois))))
	
	if(max_time == Inf)
		max_time_condition <- ""
	else
		max_time_condition <-  sprintf("AND t < %e", max_time * 1000) 
	
	min_time <- min_time * 1000 # to ms
	
	sql_query_fun <- function(i){
		print(i)
		sql_query <- sprintf("SELECT * FROM ROI_%i WHERE t >= %e %s",i,min_time, max_time_condition )
		roi_dt <- as.data.table(dbGetQuery(con, sql_query))
		roi_dt[, id := NULL]
		roi_dt[, roi_id := i]
		roi_row <- roi_map[i]
		
		if(!is.null(reference_hour)){
			p <- metadata$date_time
			hour_start <- as.numeric(format(p, "%H")) + as.numeric(format(p, "%M")) / 60 +  as.numeric(format(p, "%S")) / 3600
			ms_after_ref <- ((hour_start - reference_hour) %% 24) * 3600 * 1000
			roi_dt[, t:= (t + ms_after_ref) ]
		}
		
		if(time_in_seconds)
			roi_dt[, t:= t/1e3]
		
		roi_width <- max(c(roi_row[,w], roi_row[,h]))
		for(var_n in var_map$var_name){
			if(var_map[var_n, functional_type] == "distance"){
				roi_dt[, (var_n) := get(var_n) / roi_width]
			}
			if(var_map[var_n, sql_type] == "BOOLEAN"){
				roi_dt[, (var_n) := as.logical(get(var_n))]
			}
		}
		return(roi_dt)
	}

	out <- lapply(rois, sql_query_fun)
	dbDisconnect(con)
	
	if(!is.null(FUN)){
		out <- lapply(out, FUN, ...)
	}

	
	out <- rbindlist(out)
	

	if(add_file_name){
		out[, file := basename(FILE)]
		setkeyv(out, c("file", "roi_id"))
	}
	else{
		setkeyv(out, "roi_id")
	}
	
	
	if(!is.null(condition_df)){
		condition_df <- as.data.table(condition_df)
		out <- out[condition_df]
		}
		
	return(out)		
}


NULL
#' Get metadata from a result file.
#' 
#' This function is used to obtain meta data -- such as `time and date of the experiment' , `aquisition device', `version of the software' and others--
#' contained in a result file generated by PSV.
#'
#' @param FILE the name of the input file.
#' @return A list containing fields for metadata entries
#' @examples
#' \dontrun{
#' FILE <- "result.db"
#' out <- loadMetaData(FILE)
#' names(out)
#' }
#' @seealso \code{\link{loadROIsFromFile}} to obtain raw experiemental data. 
#' @export
loadMetaData <- function(FILE){
	con <- dbConnect(SQLite(), FILE)
	metadata <- dbGetQuery(con, "SELECT * FROM METADATA")
	dbDisconnect(con)
	v <- as.list(metadata$value)
	names(v) <- metadata$field
	#fixme explicitly GMT
	v$date_time <- as.POSIXct(as.integer(v$date_time),origin="1970-01-01",tz = "GMT")
	return(v)		
	}
	
#@include 
NULL
#' Read multiple files into a single data table
#' 
#' This function is used to conveniently put together data from different monitors/experiemnts.
#'
#' @param files either a vector of files or a data.table (or data frame). When \code{files} is a data.table, it \emph{must} have, at least, a column named `file'. 
#' The other columns can be used to map experimental variables (e.g. treatment/genotype/...) to each file.
#' @param ... further arguments for \code{\link{loadROIsFromFile}}
#' @return A data.table where each row is a unique measurment (i.e. one position at one time, for one animal).
#' @note When loading multiple files using \code{loadMultipleFiles}, the filename (i.e. column `file') is used as additional key for the data.table.
#' Conceptually, this means that time series are for every combination of ROI id (animal) \emph{and} file name (experiment).
#' @examples
#' \dontrun{
#' # The paths of the four files we want to load
#' path <- c(
#' 		"2015-04-17_17-06-49_00016dfce6e94dee9bb1a845281b086e.db",
#' 		"2015-04-17_17-09-18_00026dfce6e94dee9bb1a845281b086e.db",
#' 		"2015-04-17_17-10-32_00036dfce6e94dee9bb1a845281b086e.db",
#' 		"2015-04-17_17-11-11_00046dfce6e94dee9bb1a845281b086e.db"
#' 	)
#' # In the same order, experiemental conditions that were applied to these four experiements.
#' conditions <- as.factor(c("ctrl", "cond_1", "ctrl", "cond_1"))
#' # We build a table where every "path" maps a conditions.
#' ref <- data.table(path,conditions)
#' # We load rois 1 to 16 from all the files.Since Experiments have 
#' # not started at exactly the same time, we align them to a reference hour (9:00).
#' # We also average (interpolate) every 5s of data
#' dt <- loadMultipleFiles(files=ref, rois=c(1:16), reference_hour = 9, FUN=interpolateROIData, fs=1/5.)
#' }
#' @seealso \code{\link{loadROIsFromFile}} to load single files.
#' @export
loadMultipleFiles <- function(files, ...){
	path <- files
	file_dt <- as.data.table(path)
	if(!("path" %in% colnames(file_dt)))
		stop("file_dt should be a dataframe with, at least, a column names 'path'")
	
	file_dt[,file := basename(path)]
	setkey(file_dt, file)
	#print(file_dt[,file])
	dup <- anyDuplicated(file_dt[,file])
	if(dup != 0)
		stop(
			sprintf("Duplicated file name: %s",file_dt[dup,file])
			)
			
	l_dt <- lapply(file_dt[,path], function(x){
			loadROIsFromFile(x, add_file_name=T, ...)
		})
	
	if(length(unique(lapply(l_dt,key))) > 1){
		stop("Data tables do not have the same keys")
		}
	keys <- key(l_dt[[1]])
	
	out <- rbindlist(l_dt)
	rm(l_dt)
	
	setkeyv(out, keys)
	out <- file_dt[out]
	setkeyv(out, keys)
	return(out)
	} 

NULL

loadDAMFiles <- function(FILES, channels = NULL, min_time = 0, max_time = Inf, interval = 60){
	### hardcodded constants
	DAM_COL_TYPES <- c(
		c("integer", "character", "character", "character", "character"),
		rep("NULL", 7), ## these columns are irrelevant, NULL means we will discard them straight away
		rep("double", 32)
		)

	DAM_COL_NAMES <- c("idx", "day", "month", "year", "time",rep(NA, 7), sprintf("channel_%02d", 1:32))
	
	# todo sort files per actual dates before concat
	df_list <- lapply(FILES, function(f){
			read.table(f, colClasses = DAM_COL_TYPES, header = FALSE, col.names=DAM_COL_NAMES)
		})

	df <- do.call("rbind", df_list)
	#quick fix
	df$t <- 1:nrow(df) * interval #(s)
	df$day <- NULL
	df$month <- NULL
	df$year <- NULL
	df$time <- NULL
	
	if(is.null(channels))
		channels <- 1:32
	
	df <- subset(df, t > min_time & t < max_time)
	chans_to_fecth <- sprintf("channel_%02d", channels)
	
	df_l <- lapply(chans_to_fecth, function(x){
			data.frame(t=df$t,activity=df[,x])
		})
	names(df_l) <- chans_to_fecth	
	return(df_l)
}	

