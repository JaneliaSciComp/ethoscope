#@include 
NULL
#' Read data from aresult file.
#' 
#' This function is used to convert all the information
#' contained in a result file generated by PSV into R dataframe(s).
#'
#' @param FILE the name of the input file.
#' @param rois a vector of ROI indices to read. NULL returns all ROIs
#' @param min_time exclude data before min_time (in seconds)
#' @param max_time exclude data after max_time (in seconds)
#' @param relative_distance whether distance is converted between 0 and 1 relatively to the width of the ROI. If FALSE, varibales such as x,y, w and h are returned is returned in absolute number of pixels.
#' @param FUN an optionnal function to be applied to the resulting dataframe. It can be used, for instance, to compute summary statistics, or to transform the data.
#' @param ... extra arguments to be passed to \code{FUN}
#' @return If \code{rois} has only one element, a dataframe. Otherwise, a list of dataframes (one per ROI)
#' @note Analysis of many long (sevaral days) recording can use a lot of memory. 
#' Therefore, it can be advantageaous to load an process ROIs one by one.
#' @examples
#' \dontrun{
#' FILE <- "result.db"
#' out <- loadROIsFromFile(FILE,c(1,3,55))
#' #histogram of x marginal distribution
#' hist(out$ROI_1$x, nclass=100)
#' }
#' \dontrun{
#' ####### Using the FUN argument to resample data as it is loaded.
#' # This is preferable for very large dataset, or when owrking with
#' # little RAM.
#' # First, we compute the last time point available for all ROIs
#' out <- loadROIsFromFile(FILE, FUN=function(d)max(d$t))
#' max_t <- max(unlist(out))
#'
#' # Then we apply the interpolateROIData to all ROIs
#'  out <- loadROIsFromFile(FILE, 
#' 	FUN=interpolateROIData, start=0,
#' 	stop = max_t, fs=1)
#' 	}
#' @seealso \code{\link{loadMetaData}} To display global informations about the experiment. 
#' @export
loadROIsFromFile <- function(FILE, rois = NULL, min_time = 0, max_time = Inf, relative_distances = TRUE, FUN=NULL, ...){
	
	time_in_seconds <- TRUE

	con <- dbConnect(SQLite(), FILE)
	roi_map <- dbGetQuery(con, "SELECT * FROM ROI_MAP")
	var_map <- dbGetQuery(con, "SELECT * FROM VAR_MAP")
	
	rownames(roi_map) <- roi_map$roi_idx
	rownames(var_map) <- var_map$var_name
	
	available_rois  <- roi_map$roi_idx
	if(is.null(rois))
		rois <- available_rois
	
	matched <- rois %in% available_rois
	
	unmatched_idx <-  which(!matched)
	
	for(i in rois[unmatched_idx])
		warning(sprintf("Roi %i is not in the table", i))
	
	
	rois <- rois[matched]
	
	if (length(rois) == 0)
		stop(sprintf("No ROI to be read. available ROIs are: %s", str(sort(available_rois))))
	
	if(max_time == Inf)
		max_time_condition <- ""
	else
		max_time_condition <-  sprintf("AND t < %e", max_time * 1000) 
	
	min_time <- min_time * 1000 # to ms
	
	sql_query_fun <- function(i){
		sql_query <- sprintf("SELECT * FROM ROI_%i WHERE t >= %e %s",i,min_time, max_time_condition )
		roi_dt <- as.data.table(dbGetQuery(con, sql_query)	)
		roi_dt$id <- NULL
		roi_row <- subset(roi_map, roi_idx == i)
		
		if(time_in_seconds)
			roi_dt[, t:= t/1e3]
		
		
		w <- max(c(roi_row[1,"w"], roi_row[1,"h"]))
		for(var_n in var_map$var_name){
			if(var_map[var_n, "functional_type"] == "distance")
				roi_dt[,var_n := var_n/w]
			if(var_map[var_n, "sql_type"] == "BOOLEAN")
				roi_dt[,var_n := as.logical(var_n)]
		}
			
		if(!is.null(FUN))
			return(FUN(roi_dt, ...))
		return(roi_dt)
	}
	out <- lapply(rois, sql_query_fun)
	dbDisconnect(con)
	
	names(out) <- paste0("ROI_", rois)
	
	if(length(out) == 1)
		return (out[[1]])
		
	return(out)		
}
NULL
#' Get metadata from a result file.
#' 
#' This function is used to obtain meta data -- such as `time and date of the experiment' , `aquisition device', `version of the software' and others--
#' contained in a result file generated by PSV.
#'
#' @param FILE the name of the input file.
#' @return A list containing fields for metadata entries
#' @examples
#' \dontrun{
#' FILE <- "result.db"
#' out <- loadMetaData(FILE)
#' names(out)
#' }
#' @seealso \code{\link{loadROIsFromFile}} to obtain raw experiemental data. 
#' @export
loadMetaData <- function(FILE){
	con <- dbConnect(SQLite(), FILE)
	metadata <- dbGetQuery(con, "SELECT * FROM METADATA")
	dbDisconnect(con)
	v <- as.list(metadata$value)
	names(v) <- metadata$field
	#fixme explicitly GMT
	v$date_time <- as.POSIXct(as.integer(v$date_time),origin="1970-01-01")
	return(v)		
	}
	

loadDAMFiles <- function(FILES, channels = NULL, min_time = 0, max_time = Inf, interval = 60){
	### hardcodded constants
	DAM_COL_TYPES <- c(
		c("integer", "character", "character", "character", "character"),
		rep("NULL", 7), ## these columns are irrelevant, NULL means we will discard them straight away
		rep("double", 32)
		)

	DAM_COL_NAMES <- c("idx", "day", "month", "year", "time",rep(NA, 7), sprintf("channel_%02d", 1:32))
	
	# todo sort files per actual dates before concat
	df_list <- lapply(FILES, function(f){
			read.table(f, colClasses = DAM_COL_TYPES, header = FALSE, col.names=DAM_COL_NAMES)
		})

	df <- do.call("rbind", df_list)
	#quick fix
	df$t <- 1:nrow(df) * interval #(s)
	df$day <- NULL
	df$month <- NULL
	df$year <- NULL
	df$time <- NULL
	
	if(is.null(channels))
		channels <- 1:32
	
	df <- subset(df, t > min_time & t < max_time)
	chans_to_fecth <- sprintf("channel_%02d", channels)
	
	df_l <- lapply(chans_to_fecth, function(x){
			data.frame(t=df$t,activity=df[,x])
		})
	names(df_l) <- chans_to_fecth	
	return(df_l)
}	

